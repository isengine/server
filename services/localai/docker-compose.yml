version: "3"

services:

  localai:
    image: localai/localai:latest-gpu-nvidia-cuda-12
    environment:
      - DEBUG=true
      - NVIDIA_VISIBLE_DEVICES=all
      # - HUGGING_FACE_HUB_TOKEN=
      # - NVIDIA_VISIBLE_DEVICES=0
      # - NCCL_IGNORE_DISABLED_P2P=1
    restart: unless-stopped
    runtime: nvidia
    env_file:
      - ../../.env
    # volumes:
    #   - ./data:/root/.ollama
    volumes:
      - ./data:/build/models
    # - ./data/workspace:/workspace
    # - ./data/vllm-workspace:/vllm-workspace
    # command: ["--model", "Qwen/Qwen2-VL-7B-Instruct", "--host", "0.0.0.0", "--port", "8000"]
    # ipc: host
    # entrypoint: python3 -m vllm.entrypoints.openai.api_server
    # command: --port=8000 --host=0.0.0.0 --model=Qwen/Qwen2-VL-7B-Instruct
    # command: -m vllm.entrypoints.openai.api_server --port=11410 --host=0.0.0.0 ${H2OGPT_VLLM_ARGS}
    # command: -m vllm.entrypoints.openai.api_server --port=8000 --host=0.0.0.0 --model=Qwen/Qwen2-VL-7B-Instruct
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
    #   interval: 1m
    #   timeout: 20m
    #   retries: 5
    ports:
      - 11420:8080
      # - 8080:8080
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              count: all
              # count: 1
              # options:
              #   com.nvidia.cuda.visible_devices: "0"
    networks:
      - network

networks:
  network:
    name: prod_network
    external: true
